{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":["#%md\n","123\n"]}},"colab":{"name":"demo v1.1 (tienchi working on).ipynb","provenance":[{"file_id":"1pQB5LJWd1wUOuRCE5ZqOhAsKaTvPvb4P","timestamp":1603226955104}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Anb1jlgqW2pT"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"cC3pswv8yClq","executionInfo":{"status":"ok","timestamp":1605037483236,"user_tz":300,"elapsed":342278,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"3a49e315-8619-4b27-b1cf-ba87da96f0ac","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Mount data\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"frecL70Hk8zv","executionInfo":{"status":"ok","timestamp":1605037483437,"user_tz":300,"elapsed":342451,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"214de63a-edb1-49b8-f7ab-332e6592fe24","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Files' location, set to your object-detection-zoo-master repository\n","%cd 'drive/My Drive/Colab Notebooks/object-detection-zoo-master/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/object-detection-zoo-master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CTqGpcFZXwGF","executionInfo":{"status":"ok","timestamp":1605037855996,"user_tz":300,"elapsed":714098,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"0c6d4f39-6823-4826-ce5a-406e92758b58","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Run this Cell when start a new kernel\n","# This cell can be commented out after running once\n","# Set up vertual environment\n","!pip install virtualenv\n","!virtualenv -p /usr/bin/python3 bigdata\n","!source bigdata/bin/activate\n","\n","# Fix bugs and install dependencies\n","!pip uninstall -y tensorflow\n","!pip install -r requirements.txt\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting virtualenv\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/f3/c064343ac58d1a54c393a3f66483a29500f644a5918deeb935d28673edd9/virtualenv-20.1.0-py2.py3-none-any.whl (4.9MB)\n","\u001b[K     |████████████████████████████████| 4.9MB 5.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from virtualenv) (3.3.0)\n","Requirement already satisfied: importlib-metadata<3,>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from virtualenv) (2.0.0)\n","Collecting appdirs<2,>=1.4.3\n","  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from virtualenv) (1.15.0)\n","Collecting distlib<1,>=0.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/0a/490fa011d699bb5a5f3a0cf57de82237f52a6db9d40f33c53b2736c9a1f9/distlib-0.3.1-py2.py3-none-any.whl (335kB)\n","\u001b[K     |████████████████████████████████| 337kB 57.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from virtualenv) (3.0.12)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources>=1.0; python_version < \"3.7\"->virtualenv) (3.4.0)\n","Installing collected packages: appdirs, distlib, virtualenv\n","Successfully installed appdirs-1.4.4 distlib-0.3.1 virtualenv-20.1.0\n","created virtual environment CPython3.6.9.final.0-64 in 19454ms\n","  creator CPython3Posix(dest=/content/drive/My Drive/Colab Notebooks/object-detection-zoo-master/bigdata, clear=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==20.2.3, pip==20.2.4, setuptools==50.3.0, setuptools==50.3.1, setuptools==50.3.2, wheel==0.35.1\n","  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n","Uninstalling tensorflow-2.3.0:\n","  Successfully uninstalled tensorflow-2.3.0\n","Collecting numpy==1.19.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n","\u001b[K     |████████████████████████████████| 14.5MB 198kB/s \n","\u001b[?25hCollecting tqdm==4.49.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n","\u001b[?25hCollecting tensorflow-gpu==1.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n","\u001b[K     |████████████████████████████████| 377.0MB 23kB/s \n","\u001b[?25hCollecting keras==2.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n","\u001b[K     |████████████████████████████████| 317kB 40.2MB/s \n","\u001b[?25hCollecting Pillow==7.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 31.7MB/s \n","\u001b[?25hCollecting matplotlib==3.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/d6/8c4dfb23151d5a494c66ebbfdb5c8c433b44ec07fae52da5939fcda0943f/matplotlib-3.3.2-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\n","\u001b[K     |████████████████████████████████| 11.6MB 52.0MB/s \n","\u001b[?25hCollecting torch==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n","\u001b[K     |████████████████████████████████| 748.9MB 22kB/s \n","\u001b[?25hCollecting cupy-cuda100==7.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/d5/e045963d8412e933fdc78dd31dce1be2376226734dfeda6a64379617a704/cupy_cuda100-7.1.1-cp36-cp36m-manylinux1_x86_64.whl (382.9MB)\n","\u001b[K     |████████████████████████████████| 382.9MB 44kB/s \n","\u001b[?25hCollecting scikit-image==0.17.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n","\u001b[K     |████████████████████████████████| 12.4MB 239kB/s \n","\u001b[?25hCollecting torchvision==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/e6/a564eba563f7ff53aa7318ff6aaa5bd8385cbda39ed55ba471e95af27d19/torchvision-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n","\u001b[K     |████████████████████████████████| 8.8MB 1.9MB/s \n","\u001b[?25hRequirement already satisfied: easydict==1.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (1.9)\n","Collecting tensorboardX==2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 61.5MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (1.12.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (0.2.0)\n","Collecting tensorboard<1.15.0,>=1.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 51.5MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (3.12.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (1.1.0)\n","Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n","\u001b[K     |████████████████████████████████| 491kB 58.1MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (0.3.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (0.35.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (0.10.0)\n","Collecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (1.33.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (0.8.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->-r requirements.txt (line 4)) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->-r requirements.txt (line 4)) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->-r requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 6)) (0.10.0)\n","Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 6)) (2020.6.20)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 6)) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 6)) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100==7.1.1->-r requirements.txt (line 8)) (0.5)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirements.txt (line 9)) (2.4.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirements.txt (line 9)) (1.1.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirements.txt (line 9)) (2020.9.3)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirements.txt (line 9)) (2.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (50.3.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (3.3.3)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image==0.17.2->-r requirements.txt (line 9)) (4.4.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0->-r requirements.txt (line 3)) (3.4.0)\n","\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, tqdm, tensorboard, tensorflow-estimator, keras-applications, tensorflow-gpu, keras, Pillow, matplotlib, torch, cupy-cuda100, scikit-image, torchvision, tensorboardX\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","  Found existing installation: Pillow 7.0.0\n","    Uninstalling Pillow-7.0.0:\n","      Successfully uninstalled Pillow-7.0.0\n","  Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","  Found existing installation: scikit-image 0.16.2\n","    Uninstalling scikit-image-0.16.2:\n","      Successfully uninstalled scikit-image-0.16.2\n","  Found existing installation: torchvision 0.8.1+cu101\n","    Uninstalling torchvision-0.8.1+cu101:\n","      Successfully uninstalled torchvision-0.8.1+cu101\n","Successfully installed Pillow-7.2.0 cupy-cuda100-7.1.1 keras-2.2.4 keras-applications-1.0.8 matplotlib-3.3.2 numpy-1.19.2 scikit-image-0.17.2 tensorboard-1.14.0 tensorboardX-2.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0 torch-1.2.0 torchvision-0.4.0 tqdm-4.49.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","matplotlib","mpl_toolkits","numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"-kiawxaWfrIG"},"source":["from general_utils.processing import letterbox_image_padded, decode_detection_raw\n","from general_utils.visualization import visualize_detection\n","\n","import json\n","import cv2\n","import time\n","from PIL import Image\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.misc\n","colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrah9x1sjzW_"},"source":["# If meet wierd tensorflow bugs\n","# Try to unintall and install tensorflow-gpu \n","\n","# !pip uninstall tensorflow-gpu==1.14.0\n","# !pip install tensorflow-gpu==1.14.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YN8qtCT3L5NK"},"source":["# Convert video to images"]},{"cell_type":"code","metadata":{"id":"mx1gEKyXEJ89"},"source":["# Conver video to images \n","# output files located at ./assets/image\n","# frameRate decide the fps \"\"\"\n","\n","import cv2\n","vidcap = cv2.VideoCapture('man-walking-with-a-laggage.mov')\n","def getFrame(sec):\n","    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n","    hasFrames,image = vidcap.read()\n","    if hasFrames:\n","        cv2.imwrite(\"./assets/image\"+str(count)+\".jpg\", image)     # save frame as JPG file\n","    return hasFrames\n","sec = 0\n","frameRate = 1 # frames per second\n","count=1\n","success = getFrame(sec)\n","while success:\n","    count = count + 1\n","    sec = sec + frameRate\n","    sec = round(sec, 2)\n","    success = getFrame(sec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"is_executing":false},"id":"PZyF8UmzW2pV"},"source":["# Load a test image\n","\n","# fpath = './assets/dog.jpg'\n","# input_img = Image.open(fpath)\n","\n","# img = cv2.imread('./assets/dog.jpg', cv2.IMREAD_UNCHANGED)\n","# cv2_imshow(img)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbpbIR1K6i79"},"source":["# Load video\n","vidcap = cv2.VideoCapture('president-on-tech-green.mov')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSfPkBo14HuR"},"source":["# Functions\n","\n","def getFrame(sec):\n","  vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n","  hasFrames,img = vidcap.read()\n","  return (img, hasFrames)\n","\n","def generateCv2img(cv2img, detection_processed, path = None, display = False):\n","  fontScale = 0.7 # font & text block ratio\n","  for box in detection_processed:\n","    id, label, conf = box[0], box[1], box[2]\n","    x1, y1, x2, y2 = box[3], box[4], box[5], box[6]\n","    cv2.rectangle(cv2img,(x1,y1),(x2,y2),(0,255,0),6)\n","\n","    labelSize=cv2.getTextSize(label,cv2.FONT_HERSHEY_COMPLEX,fontScale,2)\n","    _x1 = x1\n","    _y1 = y1\n","    _x2 = _x1 + int(labelSize[0][0] * fontScale)\n","    _y2 = y1 - int(labelSize[0][1]* fontScale)\n","    cv2.rectangle(cv2img,(_x1,_y1),(_x2,_y2),(0,255,0),cv2.FILLED)\n","    cv2.putText(cv2img,label,(x1,y1),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0),1)\n","\n","  if path != None:\n","    cv2.imwrite(path + str(count) + \".jpg\", cv2img)\n","\n","  if display: \n","    cv2_imshow(cv2img)\n","\n","  return cv2img\n","\n","  cv2.destroyAllWindows()\n","\n","\n","def generateNoPredCv2img(cv2img, path = None, display = False):\n","  # fontScale = 0.7 # font & text block ratio\n","  # for box in detection_processed:\n","  #   id, label, conf = box[0], box[1], box[2]\n","  #   x1, y1, x2, y2 = box[3], box[4], box[5], box[6]\n","  #   cv2.rectangle(cv2img,(x1,y1),(x2,y2),(0,255,0),6)\n","\n","  #   labelSize=cv2.getTextSize(label,cv2.FONT_HERSHEY_COMPLEX,fontScale,2)\n","  #   _x1 = x1\n","  #   _y1 = y1\n","  #   _x2 = _x1 + int(labelSize[0][0] * fontScale)\n","  #   _y2 = y1 - int(labelSize[0][1]* fontScale)\n","  #   cv2.rectangle(cv2img,(_x1,_y1),(_x2,_y2),(0,255,0),cv2.FILLED)\n","  #   cv2.putText(cv2img,label,(x1,y1),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0),1)\n","\n","  if path != None:\n","    cv2.imwrite(path + str(count) + \".jpg\", cv2img)\n","\n","  if display: \n","    cv2_imshow(cv2img)\n","\n","  return cv2img\n","\n","  cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImcG5jaEW2pY"},"source":["# YOLOv3 (Darknet53) - template"]},{"cell_type":"code","metadata":{"id":"ZWnVHrkd1xR-"},"source":["from detectors.yolov3 import YOLOv3_Darknet53\n","\n","# Load the model\n","detector = YOLOv3_Darknet53(weights='model_weights/YOLOv3_VOC0712_Darknet53.h5')\n","\n","# Parameters\n","sec = 0\n","fps = 24\n","frameRate = 1 / fps # interval between frames\n","\n","count = 0\n","success = getFrame(sec)\n","images = []\n","cv2imgs = []\n","times = []\n","results = []\n","result_imgs = []\n","\n","modelName = \"yolov3dark\"\n","path = './output/'+ modelName + str(fps) + '/'\n","noPredPath = './output/'+ modelName + str(fps) + 'NoPred/'\n","\n","\n","  # detection_raw = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n","# Convert the video to frames and make prediction\n","while success:\n","  count = count + 1\n","  sec = sec + frameRate\n","  sec = round(sec, 2)\n","  image ,success = getFrame(sec)\n","  \n","  if not success:\n","    continue\n","\n","  # convert np array back to image\n","  cv2img = image\n","  cv2imgs.append(image)\n","  image = Image.fromarray(image) \n","  images.append(image)\n","\n","  # Time and make prediction\n","  x_query, x_meta = letterbox_image_padded(image, size=detector.model_img_size)\n","  start = time.time()\n","  detection_raw = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n","  end = time.time()\n","  detection_processed = decode_detection_raw(detection_raw, x_meta, detector.classes)\n","  \n","  # Generate and save the result image\n","  result_img = generateCv2img(cv2img, detection_processed, path = path, display = False)\n","  result_imgs.append(result_img)\n","\n","  Generate no prediction images\n","  result_img = generateNoPredCv2img(cv2img, noPredPath, display=False)\n","\n","  # Collect the result\n","  results.append(detection_processed)\n","  times.append(end - start)\n","\n","\n","\n","# # Time summary    \n","summary = [max(times), min(times), sum(times)/len(times) ]\n","print(\"Prediction time list\", times)\n","print(\"Max:\", max(times))\n","print(\"Min:\", min(times))\n","print(\"Avg:\", sum(times)/len(times))\n","\n","# # Save intervals \n","data = json.dumps(times)\n","\n","with open(path + 'intervals.json', 'w') as f:\n","    json.dump(data, f)\n","# Save summary\n","data = json.dumps(summary)\n","with open(path + 'time_summary.json', 'w') as f:\n","    json.dump(data, f)\n","\n","  \n","# # Confidence summary\n","conf = []\n","for elm in results:\n","  if len(elm) != 0:\n","    conf.append(elm[0][2])\n","\n","confFps = {}\n","confFps['rawConfList'] = conf\n","confFps['maxConf'] = max(conf)\n","confFps['minConf'] = min(conf)\n","confFps['avgConf'] = sum(conf)/len(conf)\n","print('max(conf)', confFps['maxConf'])\n","print('min(conf)', confFps['minConf'])\n","print('avgConf', confFps['avgConf'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QD7eEPNhY4CS","executionInfo":{"status":"ok","timestamp":1603757387180,"user_tz":240,"elapsed":431,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"046b6d9a-5f34-4625-9542-0b8a39a05c31","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["results[110]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(14, 'person', 0.9977402687072754, 67, 189, 137, 358),\n"," (14, 'person', 0.9973552227020264, 468, 71, 721, 457)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"gMTzOxKGW2pb"},"source":["# YOLOv3 (MobileNetV1)"]},{"cell_type":"code","metadata":{"id":"uqyMdxTfW2pb"},"source":["# from detectors.yolov3 import YOLOv3_MobileNetV1\n","\n","# start = time.time()\n","# detector = YOLOv3_MobileNetV1(weights='model_weights/YOLOv3_VOC0712_MobileNetV1.h5')\n","# afterLoading = time.time()\n","\n","# x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n","# afterLetterbox = time.time()\n","\n","# detection_raw = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n","# afterDetect = time.time()\n","\n","# detection_processed = decode_detection_raw(detection_raw, x_meta, detector.classes)\n","# afterProcessResult = time.time()\n","\n","# visualize_detection(input_img, detection_processed)\n","# afterVisualization = time.time()\n","\n","# print(\"Load detector loading\", afterLoading - start)\n","# print(\" padding\", afterLetterbox - afterLetterbox)\n","# print(\" detection\", afterDetect - afterLetterbox)\n","# print(\" process result\", afterProcessResult - afterDetect)\n","# print(\" visualization\", afterVisualization - afterProcessResult)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MT2TtzGwW2pe"},"source":["# SSD300 (VGG16)"]},{"cell_type":"code","metadata":{"id":"FyhFONsLW2pe"},"source":["# from detectors.ssd import SSD300\n","# detector = SSD300(weights='model_weights/SSD_VOC0712_VGG16_300x300.h5')\n","\n","# x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n","# detection_raw = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n","# detection_processed = decode_detection_raw(detection_raw, x_meta, detector.classes)\n","\n","# visualize_detection(input_img, detection_processed)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGo5QenIW2ph"},"source":["# SSD512 (VGG16)"]},{"cell_type":"code","metadata":{"id":"CY2YFIDRW2pi"},"source":["# from detectors.ssd import SSD512\n","# detector = SSD512(weights='model_weights/SSD_VOC0712_VGG16_512x512.h5')\n","\n","# x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n","# detection_raw = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n","# detection_processed = decode_detection_raw(detection_raw, x_meta, detector.classes)\n","\n","# visualize_detection(input_img, detection_processed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aM5ktsyoW2pk"},"source":["# Faster R-CNN (VGG16)"]},{"cell_type":"code","metadata":{"id":"_XUH_dMKW2pl","executionInfo":{"status":"ok","timestamp":1603256413781,"user_tz":240,"elapsed":328005,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"08504ac9-50ac-4c7d-8dc0-43c1f702e737","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["from detectors.frcnn import FRCNN\n","\n","# Load models and videos\n","detector = FRCNN().cuda(device=0).load('model_weights/FRCNN_VOC0712_VGG16.pth')\n","vidcap = cv2.VideoCapture('man-walking-with-a-laggage.mov')\n","\n","def getFrame(sec):\n","  vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n","  hasFrames,img = vidcap.read()\n","  return (img, hasFrames)\n","\n","# Parameters\n","sec = 0\n","fps = 24\n","frameRate = 1 / fps # interval between frames\n","count = 0\n","success = getFrame(sec)\n","images = []\n","times = []\n","results = []\n","\n","# int frames = \n","\n","# Convert the video to frames and make prediction\n","while success:\n","  count = count + 1\n","  sec = sec + frameRate\n","  sec = round(sec, 2)\n","  image ,success = getFrame(sec)\n","\n","  if not success:\n","    continue\n","\n","  # convert np array back to image\n","  image = Image.fromarray(image) \n","\n","  images.append(image)\n","\n","  # Time and make prediction\n","  start = time.time()\n","  x_query, x_meta = letterbox_image_padded(image, size=detector.model_img_size)\n","  detection_raw = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n","  detection_processed = decode_detection_raw(detection_raw, x_meta, detector.classes)\n","  \n","  # Generate and save the result image\n","  plt.clf()\n","  plt.figure(figsize=(3, 3))\n","  plt.imshow(image)\n","  current_axis = plt.gca()\n","  for box in detection_processed:\n","      class_id = box[0]\n","      class_name = box[1]\n","      confidence = box[2]\n","      xmin, ymin, xmax, ymax = box[3:]\n","      color = colors[class_id]\n","      label = '{}: {:.2f}'.format(class_name, confidence)\n","      current_axis.add_patch(\n","          plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, color=color, fill=False, linewidth=2))\n","      current_axis.text(xmin, ymin, label, size='small', color='black', bbox={'facecolor': color, 'alpha': 1.0})\n","  plt.axis('off')\n","  plt.savefig('./output/frcnn' + str(fps) + '/' + str(count) + \".jpg\")\n","  plt.close()\n","  \n","  results.append(detection_processed)\n","  # Display image and print the time\n","  # plt.show() \n","  # print('Time for', count, ':', time.time() - start)\n","  times.append(time.time() - start)\n","\n","# Print results\n","print(\"Number of frames:\", len(times))\n","print(\"Prediction time list\", times)\n","print(\"Max:\", max(times))\n","print(\"Min:\", min(times))\n","print(\"Avg:\", sum(times)/len(times))\n","\n","# print(\"Prediction time list\", times[1:])\n","# print(\"Max:\", max(times[1:]))\n","# print(\"Min:\", min(times[1:]))\n","# print(\"Avg:\", sum(times[1:])/len(times[1:]))\n","\n","# Time data to json\n","timeFps24 = {}\n","timeFps24['rawTimeList'] = times\n","timeFps24['maxTime'] = max(times)\n","timeFps24['minTime'] = min(times)\n","timeFps24['avgTime'] = sum(times)/len(times)\n","\n","data = json.dumps(timeFps24)\n","\n","with open('frcnn-time-fps' + str(fps) + '.json', 'w') as f:\n","    json.dump(data, f)\n","\n","# # Confidence json example\n","# confFps24 = {}\n","# confFps24['rawConfList'] = conf\n","# confFps24['maxConf'] = max(conf)\n","# confFps24['minConf'] = min(conf)\n","# confFps24['avgConf'] = sum(conf)/len(conf)\n","\n","conf = []\n","for elm in results:\n","  if len(elm) != 0:\n","    conf.append(elm[0][2])\n","    \n","# Confidence \n","confFps = {}\n","confFps['rawConfList'] = conf\n","confFps['maxConf'] = max(conf)\n","confFps['minConf'] = min(conf)\n","confFps['avgConf'] = sum(conf)/len(conf)\n","print('max(conf)', max(conf))\n","print('min(conf)', min(conf))\n","\n","data = json.dumps(confFps)\n","\n","with open('./results/frcnn-conf-fps' + str(fps) + '.json', 'w') as f:\n","    json.dump(data, f)\n","\n","# data = json.dumps(confFps24)\n","\n","# with open('frcnn-conf-fps24.json', 'w') as f:\n","#     json.dump(data, f)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of frames: 145\n","Prediction time list [0.7449469566345215, 0.7249050140380859, 0.7237453460693359, 0.7222223281860352, 0.7468395233154297, 0.7458624839782715, 0.7431554794311523, 0.7161104679107666, 0.7244644165039062, 0.7531154155731201, 0.7579562664031982, 0.7494015693664551, 0.7190966606140137, 0.7220158576965332, 0.7349593639373779, 0.7390279769897461, 0.736419677734375, 0.7216107845306396, 0.7232699394226074, 0.718327522277832, 0.7308573722839355, 0.7524247169494629, 0.7399032115936279, 0.7774264812469482, 0.7335524559020996, 0.7379856109619141, 0.7387127876281738, 0.7364380359649658, 0.7144253253936768, 0.7308437824249268, 0.7668163776397705, 0.8882791996002197, 0.75307297706604, 0.731184720993042, 0.7388408184051514, 0.7535130977630615, 0.8558692932128906, 0.7351124286651611, 0.7292594909667969, 0.7113137245178223, 0.7226529121398926, 0.7378325462341309, 0.713914155960083, 0.7206425666809082, 0.7228696346282959, 0.7376513481140137, 0.7201616764068604, 0.7406826019287109, 0.749506950378418, 0.7479686737060547, 0.750009298324585, 0.7310407161712646, 0.7248640060424805, 0.7220604419708252, 0.7249369621276855, 0.7380497455596924, 0.7417497634887695, 0.7352867126464844, 0.7390716075897217, 0.7225861549377441, 0.7214250564575195, 0.7318694591522217, 0.7490732669830322, 0.7372744083404541, 0.8657877445220947, 0.8478670120239258, 0.744312047958374, 0.7231740951538086, 0.7299468517303467, 0.7363202571868896, 0.7220878601074219, 0.7295455932617188, 0.7224972248077393, 0.7524826526641846, 0.7414047718048096, 0.7507791519165039, 0.7296497821807861, 0.7425265312194824, 0.7291080951690674, 0.7419333457946777, 0.7397069931030273, 0.7522308826446533, 0.7191364765167236, 0.729299783706665, 0.7525241374969482, 0.7572896480560303, 0.751683235168457, 0.7284443378448486, 0.7196788787841797, 0.7436888217926025, 0.7531585693359375, 0.768582820892334, 0.7442052364349365, 0.7295396327972412, 0.7382504940032959, 0.7308919429779053, 0.758561372756958, 0.773223876953125, 0.7593123912811279, 0.7392761707305908, 0.7505745887756348, 0.7629108428955078, 0.7616410255432129, 0.7558794021606445, 0.7509775161743164, 0.7545437812805176, 0.770111083984375, 0.894167423248291, 0.7585599422454834, 0.7408807277679443, 0.777489423751831, 0.755446195602417, 0.7520661354064941, 0.9512500762939453, 0.737565279006958, 0.7586874961853027, 0.7358169555664062, 0.740588903427124, 0.85231614112854, 0.7440717220306396, 0.7461400032043457, 0.7471678256988525, 0.7412757873535156, 0.7593634128570557, 0.7540855407714844, 0.7558276653289795, 0.7703659534454346, 0.7487716674804688, 0.7484884262084961, 0.7538232803344727, 0.7465066909790039, 0.7650079727172852, 0.9222393035888672, 0.7643489837646484, 0.7391266822814941, 0.7628402709960938, 0.7859432697296143, 0.7663259506225586, 0.7546548843383789, 0.7400064468383789, 0.7286357879638672, 0.7323238849639893, 0.7434587478637695, 0.7227792739868164, 0.7903599739074707]\n","Max: 0.9512500762939453\n","Min: 0.7113137245178223\n","Avg: 0.7496550806637468\n","max(conf) 0.9995493292808533\n","min(conf) 0.7042287588119507\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"pgFAuSXrdE8g","executionInfo":{"status":"ok","timestamp":1603256555970,"user_tz":240,"elapsed":776,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"f991c3e2-03f5-4b14-dfcb-e86bd1d50806","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(\"Number of frames:\", len(times))\n","print(\"Prediction time list\", times)\n","print(\"Max:\", max(times))\n","print(\"Min:\", min(times))\n","print(\"Avg:\", sum(times)/len(times))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of frames: 145\n","Prediction time list [0.7449469566345215, 0.7249050140380859, 0.7237453460693359, 0.7222223281860352, 0.7468395233154297, 0.7458624839782715, 0.7431554794311523, 0.7161104679107666, 0.7244644165039062, 0.7531154155731201, 0.7579562664031982, 0.7494015693664551, 0.7190966606140137, 0.7220158576965332, 0.7349593639373779, 0.7390279769897461, 0.736419677734375, 0.7216107845306396, 0.7232699394226074, 0.718327522277832, 0.7308573722839355, 0.7524247169494629, 0.7399032115936279, 0.7774264812469482, 0.7335524559020996, 0.7379856109619141, 0.7387127876281738, 0.7364380359649658, 0.7144253253936768, 0.7308437824249268, 0.7668163776397705, 0.8882791996002197, 0.75307297706604, 0.731184720993042, 0.7388408184051514, 0.7535130977630615, 0.8558692932128906, 0.7351124286651611, 0.7292594909667969, 0.7113137245178223, 0.7226529121398926, 0.7378325462341309, 0.713914155960083, 0.7206425666809082, 0.7228696346282959, 0.7376513481140137, 0.7201616764068604, 0.7406826019287109, 0.749506950378418, 0.7479686737060547, 0.750009298324585, 0.7310407161712646, 0.7248640060424805, 0.7220604419708252, 0.7249369621276855, 0.7380497455596924, 0.7417497634887695, 0.7352867126464844, 0.7390716075897217, 0.7225861549377441, 0.7214250564575195, 0.7318694591522217, 0.7490732669830322, 0.7372744083404541, 0.8657877445220947, 0.8478670120239258, 0.744312047958374, 0.7231740951538086, 0.7299468517303467, 0.7363202571868896, 0.7220878601074219, 0.7295455932617188, 0.7224972248077393, 0.7524826526641846, 0.7414047718048096, 0.7507791519165039, 0.7296497821807861, 0.7425265312194824, 0.7291080951690674, 0.7419333457946777, 0.7397069931030273, 0.7522308826446533, 0.7191364765167236, 0.729299783706665, 0.7525241374969482, 0.7572896480560303, 0.751683235168457, 0.7284443378448486, 0.7196788787841797, 0.7436888217926025, 0.7531585693359375, 0.768582820892334, 0.7442052364349365, 0.7295396327972412, 0.7382504940032959, 0.7308919429779053, 0.758561372756958, 0.773223876953125, 0.7593123912811279, 0.7392761707305908, 0.7505745887756348, 0.7629108428955078, 0.7616410255432129, 0.7558794021606445, 0.7509775161743164, 0.7545437812805176, 0.770111083984375, 0.894167423248291, 0.7585599422454834, 0.7408807277679443, 0.777489423751831, 0.755446195602417, 0.7520661354064941, 0.9512500762939453, 0.737565279006958, 0.7586874961853027, 0.7358169555664062, 0.740588903427124, 0.85231614112854, 0.7440717220306396, 0.7461400032043457, 0.7471678256988525, 0.7412757873535156, 0.7593634128570557, 0.7540855407714844, 0.7558276653289795, 0.7703659534454346, 0.7487716674804688, 0.7484884262084961, 0.7538232803344727, 0.7465066909790039, 0.7650079727172852, 0.9222393035888672, 0.7643489837646484, 0.7391266822814941, 0.7628402709960938, 0.7859432697296143, 0.7663259506225586, 0.7546548843383789, 0.7400064468383789, 0.7286357879638672, 0.7323238849639893, 0.7434587478637695, 0.7227792739868164, 0.7903599739074707]\n","Max: 0.9512500762939453\n","Min: 0.7113137245178223\n","Avg: 0.7496550806637468\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RyNhc5EHSAmF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EdPFPxdUR5kN"},"source":["# Convert images back to video\n","import cv2\n","import numpy as np\n","import glob\n"," \n","img_array = []\n","for filename in  glob.glob('./output/frcnn' + str(24) + '/*.jpg'):\n","    img = cv2.imread(filename)\n","    height, width, layers = img.shape\n","    size = (width,height)\n","    img_array.append(img)\n"," \n"," \n","out = cv2.VideoWriter('test.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 24, size)\n"," \n","for i in range(len(img_array)):\n","    out.write(img_array[i])\n","out.release()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWmILDs4b5eL","executionInfo":{"status":"error","timestamp":1603252202706,"user_tz":240,"elapsed":431353,"user":{"displayName":"Tienchi Hsin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTNkQbuzZtJLpS66Kpyl4Cb7rpNaf2a60eG56B42o=s64","userId":"06277019657017912781"}},"outputId":"577c4bfd-a883-49a0-920e-23bce0f8a316","colab":{"base_uri":"https://localhost:8080/","height":130}},"source":[""],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-c6d0aaed1752>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    frameRate = 1/ # interval between frames\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"rNxm5CsxRvy3"},"source":["timeFps24 = {}\n","timeFps24['rawTimeList'] = times\n","timeFps24['maxTime'] = max(times)\n","timeFps24['minTime'] = min(times)\n","timeFps24['avgTime'] = sum(times)/len(times)\n","\n","data = json.dumps(timeFps24)\n","\n","with open('frcnn-time-fps24.json', 'w') as f:\n","    json.dump(data, f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1G9cbMdYXqOF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5QWsAJkvxX-"},"source":["# # >>> v.get(cv2.CAP_PROP_POS_MSEC)\n","# # 213400.0\n","# # Checking shows that this sets the point after the last frame (not before it), so the timestamp is indeed the exact total length of the stream:\n","\n","# # >>> v.get(cv2.CAP_PROP_POS_FRAMES)\n","# # 5335.0\n","# # >>>> v.get(cv2.CAP_PROP_FRAME_COUNT)\n","# # 5335.0\n","\n","# cap = cv2.VideoCapture('man-walking-with-a-laggage.mov')\n","# # vidcap.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n","# # cap.get(cv2.CAP_PROP_FPS)\n","# # vidcap.get(cv2.CAP_PROP_POS_FRAMEcv2.CAP_PROP_FPSS)\n","\n","# old_fps = cap.get(cv2.CAP_PROP_FPS)      \n","# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","# duration = frame_count/old_fps\n","# new_fps = 24\n","# new_frame_count = int(duration * new_fps)\n","\n","# print('fps = ' + str(fps))\n","# print('number of frames = ' + str(frame_count))\n","# print('duration (S) = ' + str(duration))\n","# print('new count = ' + str(new_frame_count))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ASsmtclGtZ9"},"source":["# # Convert images to video\n","\n","# # img_array = []\n","# # for filename in glob.glob('C:/New folder/Images/*.jpg'):\n","# #     img = cv2.imread(filename)\n","# #     height, width, layers = img.shape\n","# #     size = (width,height)\n","# #     img_array.append(img)\n","\n","\n","# out = cv2.VideoWriter('test.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 24, size)\n","# for i in range(len(images)):\n","#     out.write(img_array[i])\n","# out.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i03361y99SBu"},"source":["# Convert imgs to video from cv2imgs\n","\n","size = cv2imgs[0].shape[:2]\n","out = cv2.VideoWriter('test.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 24, size)\n","for i in range(len(cv2imgs)):\n","    out.write(cv2imgs[i])\n","out.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptq5DZAm9eR2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j45yERce93OC"},"source":[""],"execution_count":null,"outputs":[]}]}